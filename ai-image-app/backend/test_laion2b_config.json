{
  "training": {
    "base_model": "runwayml/stable-diffusion-v1-5",
    "resolution": 512,
    "batch_size": 1,
    "gradient_accumulation_steps": 4,
    "learning_rate": 0.0001,
    "total_samples": 400,
    "save_steps": 50,
    "eval_steps": 25,
    "logging_steps": 10,
    "mixed_precision": "fp16",
    "max_grad_norm": 1.0,
    "warmup_steps": 10,
    "lr_scheduler": "cosine"
  },
  "lora": {
    "r": 16,
    "lora_alpha": 32,
    "target_modules": [
      "to_q",
      "to_k",
      "to_v",
      "to_out.0",
      "ff.net.0.proj",
      "ff.net.2"
    ],
    "lora_dropout": 0.1,
    "bias": "none",
    "task_type": "CAUSAL_LM"
  },
  "datasets": [
    {
      "type": "huggingface",
      "name": "laion/laion2B-en",
      "split": "train",
      "streaming": true,
      "shuffle_buffer": 1000
    }
  ],
  "advanced": {
    "enable_wandb": false,
    "enable_tensorboard": false,
    "gradient_checkpointing": true,
    "use_8bit_adam": false,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "adam_weight_decay": 0.01,
    "adam_epsilon": 1e-08,
    "max_grad_norm": 1.0,
    "warmup_steps": 10,
    "lr_scheduler": "cosine",
    "resume_from_checkpoint": null,
    "max_memory_usage": 0.8
  }
}